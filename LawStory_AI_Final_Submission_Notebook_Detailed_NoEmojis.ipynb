{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5ceae2",
   "metadata": {},
   "source": [
    "# LawStory AI — Judgment PDF to Multi-Scene Explainer Video (Text-Based Prototype)\n",
    "\n",
    "This notebook documents the end-to-end AI + Automation system that converts a **court judgment PDF** into a **multi-scene explainer video** with a minimal, readable design (white text on black background). The system is designed to generate **one final video link per judgment** using a structured pipeline, without requiring manual script writing or manual video editing.\n",
    "\n",
    "The current submitted version successfully generates a **multi-scene text-based explainer video without audio**. A voiceover-enabled version is under development using **ElevenLabs** for text-to-speech and **Cloudinary** for media asset management.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e8586",
   "metadata": {},
   "source": [
    "## 1) Problem Definition & Objective\n",
    "\n",
    "### a. Selected project track\n",
    "\n",
    "AI + Automation (LLM + Video Rendering Pipeline)  \n",
    "This project falls under the **AI + Automation** track and combines two essential capabilities into one automated system:\n",
    "1. **LLM-based legal understanding**, where an AI model interprets long, unstructured legal judgments and converts them into structured explainable content.\n",
    "2. **Automated video rendering**, where the structured content is programmatically converted into a short explainer video without human editing.\n",
    "\n",
    "The core objective is to build a system where the **only input required from the user is a judgment PDF**, and the output is a structured multi-scene video that explains the case in a clear and accessible format. This directly supports the larger product vision of **LawStory AI**, where legal information is transformed into content that is faster to consume and easier to understand.\n",
    "\n",
    "### b. Clear problem statement\n",
    "\n",
    "Court judgments are one of the most important sources of legal learning and legal awareness, but they are also one of the hardest formats to consume. A judgment can be dozens or even hundreds of pages long, full of procedural details, legal reasoning, citations, and dense language. For most users, the real challenge is not simply reading the PDF, but identifying and connecting the most critical components such as:\n",
    "- the **facts of the case**\n",
    "- the **legal issues involved**\n",
    "- the **arguments from both sides**\n",
    "- the **final decision**\n",
    "- the **reasoning and ratio**\n",
    "\n",
    "The problem addressed by this project is to automatically convert a judgment PDF into a short, structured, multi-scene explainer video, without manual intervention. Without such a system, users must spend hours reading the full document, depend on scattered and inconsistent summaries, or avoid judgments entirely due to complexity and time constraints.\n",
    "\n",
    "### c. Real-world relevance and motivation\n",
    "\n",
    "This problem has strong real-world relevance because judgments impact learning, awareness, and decision-making, but they are not easily accessible in their raw form. This system solves a practical gap that traditional approaches do not solve at scale:\n",
    "- **Law students** need fast and structured revision of case law, especially for exams and internships.\n",
    "- **Common people** want simplified understanding of judgments that affect rights, family disputes, property matters, criminal matters, and constitutional issues.\n",
    "- **Lawyers and legal educators** benefit from structured explainers for teaching and awareness.\n",
    "- **Legal content creators** need consistent, scalable production of explainers without manually reading and rewriting every case.\n",
    "\n",
    "The motivation behind this pipeline is that legal information should not remain inaccessible due to complexity. By converting a judgment into a structured video format, the system improves comprehension speed and increases the reach of legal knowledge. This makes it a strong foundation for a product feature in LawStory AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1c853",
   "metadata": {},
   "source": [
    "## 2) Data Understanding & Preparation\n",
    "\n",
    "### a. Dataset source (public / synthetic / collected / API)\n",
    "\n",
    "This project does not use a fixed traditional dataset such as CSV files, labeled training datasets, or structured records. Instead, it uses **real-world live input** in the form of **court judgment PDFs**, which are provided by users as the primary input. The extracted text from these PDFs becomes the raw data used for the AI generation step.\n",
    "\n",
    "The dataset source for this project is therefore best described as:\n",
    "- **Collected / user-provided judgment PDF documents**\n",
    "- Extracted unstructured text obtained through the Make.com scenario pipeline\n",
    "\n",
    "This approach reflects real-world usage because judgments vary widely across courts, formatting, length, and complexity.\n",
    "\n",
    "### b. Data loading and exploration\n",
    "\n",
    "The data enters the system through Make.com using an automation pipeline. The key steps in data loading and exploration are:\n",
    "- The user provides a **judgment PDF** (or a judgment PDF URL).\n",
    "- The pipeline receives it through a **Webhook trigger**.\n",
    "- An **HTTP module** fetches and downloads the judgment file.\n",
    "- A **Custom JavaScript module** extracts the PDF into plain text.\n",
    "\n",
    "This stage acts as the practical “exploration and validation” stage. It ensures:\n",
    "- the PDF was fetched correctly\n",
    "- the extracted text is readable and complete enough for summarization\n",
    "- the extracted content contains key legal context required for video generation\n",
    "\n",
    "At this stage, the extracted judgment text must contain enough detail for the LLM to identify and generate structured information including:\n",
    "- facts\n",
    "- issues\n",
    "- arguments from both sides\n",
    "- decision and ratio/reasoning\n",
    "\n",
    "### c. Cleaning, preprocessing, feature engineering\n",
    "\n",
    "This project does not involve numeric feature engineering because it is not a model-training project. Instead, the preprocessing is designed to make unstructured legal text usable for an LLM and automation pipeline.\n",
    "\n",
    "The key preprocessing steps are:\n",
    "- ensuring extracted text is passed as one continuous input to Gemini\n",
    "- reducing failure points caused by formatting artifacts such as line breaks and spacing\n",
    "- structuring the output into a predictable JSON schema that Make can parse reliably\n",
    "\n",
    "The most important transformation performed is:\n",
    "**raw judgment text → structured multi-scene video script JSON**\n",
    "\n",
    "This JSON contains multiple scenes, and each scene includes:\n",
    "- narration\n",
    "- duration_seconds\n",
    "- visual instructions\n",
    "\n",
    "### d. Handling missing values or noise (if applicable)\n",
    "\n",
    "Legal PDF text extraction can contain noise and inconsistencies, including:\n",
    "- irregular line breaks\n",
    "- repeated headers/footers\n",
    "- inconsistent spacing and formatting\n",
    "- scanned PDFs with weaker extraction quality\n",
    "- missing explicit section headings\n",
    "\n",
    "This was handled practically by using Gemini to produce clean structured JSON output, rather than relying on the raw extracted formatting. The system also depends on strict JSON formatting so the output remains parseable and usable downstream in Make.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample extracted text snippet (demo placeholder)\n",
    "# In the actual pipeline, this text comes from the PDF -> text extraction step in Make.com.\n",
    "\n",
    "judgment_extracted_text = \"\"\"\n",
    "IN THE HIGH COURT OF DELHI AT NEW DELHI\n",
    "Party A v. Party B\n",
    "\n",
    "The dispute concerns residence rights in a property claimed as a shared household.\n",
    "The court examined ownership documents, contributions, and the effect of divorce on residence rights.\n",
    "The appeal was dismissed.\n",
    "\"\"\"\n",
    "\n",
    "print(judgment_extracted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad7379",
   "metadata": {},
   "source": [
    "## 3) Model / System Design\n",
    "\n",
    "### a. AI technique used (ML / DL / NLP / LLM / Recommendation / Hybrid)\n",
    "\n",
    "This project uses an LLM-based NLP generation approach combined with a video rendering system. The AI technique is primarily:\n",
    "- **LLM-based NLP generation**, where the judgment text is interpreted and transformed into structured scenes.\n",
    "- **Automation + rendering**, where the structured output is converted into a video timeline and rendered automatically.\n",
    "\n",
    "The system uses:\n",
    "- **Google Gemini** for script generation in a structured multi-scene JSON format.\n",
    "- **Shotstack API** for rendering the final video and returning a hosted output link.\n",
    "\n",
    "This makes the overall system a Hybrid AI + Media Rendering Automation pipeline.\n",
    "\n",
    "### b. Architecture or pipeline explanation\n",
    "\n",
    "The final working pipeline generates a multi-scene video with a single final output URL, without audio. The steps are as follows:\n",
    "\n",
    "1. **Webhook Trigger**  \n",
    "   Receives the judgment PDF URL/request from Bubble.\n",
    "\n",
    "2. **HTTP Module**  \n",
    "   Downloads and fetches the judgment PDF document.\n",
    "\n",
    "3. **Custom JavaScript Module**  \n",
    "   Extracts the PDF and converts it into plain text.\n",
    "\n",
    "4. **Gemini Module**  \n",
    "   Converts the extracted judgment text into a structured JSON video script.  \n",
    "   The output format includes:\n",
    "   - title_frame metadata (court name, case title, year, citation, coram)\n",
    "   - scenes array with multiple scenes\n",
    "\n",
    "5. **Parse JSON Module**  \n",
    "   Parses the Gemini output so Make can map individual fields such as narration, duration, and visuals.\n",
    "\n",
    "6. **Iterator**  \n",
    "   Iterates over the scenes array and produces one bundle per scene.\n",
    "\n",
    "7. **Text Aggregator**  \n",
    "   Combines all iterated bundles into one unified clips array for Shotstack, ensuring all scenes are included in a single render timeline.\n",
    "\n",
    "8. **Shotstack HTTP POST**  \n",
    "   Sends one render request containing all clips and receives a render ID.\n",
    "\n",
    "9. **Shotstack HTTP GET**  \n",
    "   Polls the render status until completion and returns the final video URL.\n",
    "\n",
    "This design ensures the output is not fragmented into multiple videos. Instead, the system generates one complete explainer video containing all scenes sequentially.\n",
    "\n",
    "### c. Justification of design choices\n",
    "\n",
    "This design was chosen because it provides the most stable and scalable automation workflow:\n",
    "- Legal judgments are unstructured, so Gemini is essential to interpret and restructure them into usable scenes.\n",
    "- JSON output is required because Make needs predictable keys for mapping and automation reliability.\n",
    "- Iterator and aggregator are necessary because the system produces multiple scenes, but Shotstack requires a single combined timeline to render one final video.\n",
    "- Shotstack is suitable because it supports programmatic video rendering and returns a hosted output link automatically, making the pipeline fully automated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd1548a",
   "metadata": {},
   "source": [
    "## 4) Core Implementation\n",
    "\n",
    "### a. Model training / inference logic\n",
    "\n",
    "No model training was performed in this project. The system is inference-only, meaning:\n",
    "- Gemini generates a structured script directly from extracted judgment text.\n",
    "- Shotstack renders the video directly from the structured clip timeline.\n",
    "\n",
    "This makes the pipeline lightweight, fast to iterate, and suitable for real product workflows.\n",
    "\n",
    "### b. Prompt engineering (for LLM-based projects)\n",
    "\n",
    "Prompt engineering is a key component of making the system stable. Gemini must produce output that is machine-readable and consistent, because the entire automation pipeline depends on it.\n",
    "\n",
    "The prompt was engineered to enforce:\n",
    "- output must be valid JSON only\n",
    "- no markdown or formatting wrappers\n",
    "- scenes must be an array\n",
    "- each scene must contain required keys: scene_number, duration_seconds, narration, visual\n",
    "\n",
    "This was critical because earlier failures occurred when Gemini returned:\n",
    "- extra text before/after JSON\n",
    "- markdown code fences\n",
    "- null values or unsupported keys\n",
    "\n",
    "By enforcing strict output rules, the system became stable and parseable within Make.\n",
    "\n",
    "### c. Recommendation or prediction pipeline\n",
    "\n",
    "Not applicable. This project is not a recommendation or prediction system. It is a generation pipeline that converts:\n",
    "judgment PDF text → structured scenes → video timeline → final video URL.\n",
    "\n",
    "### d. Code must run top-to-bottom without errors\n",
    "\n",
    "In the working submitted version, the pipeline runs end-to-end without errors:\n",
    "- Gemini output is parsed correctly\n",
    "- scenes are iterated properly\n",
    "- the aggregator produces a combined clips array\n",
    "- Shotstack POST succeeds\n",
    "- Shotstack GET returns status done and provides the final video link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ab16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sample Gemini output structure (demo only)\n",
    "sample_gemini_output = {\n",
    "  \"title_frame\": {\n",
    "    \"court\": \"High Court of Delhi at New Delhi\",\n",
    "    \"case_title\": \"Party A v. Party B\",\n",
    "    \"year\": \"2025\",\n",
    "    \"citation\": \"Sample Citation\",\n",
    "    \"coram\": \"Sample Coram\"\n",
    "  },\n",
    "  \"scenes\": [\n",
    "    {\n",
    "      \"scene_number\": 1,\n",
    "      \"duration_seconds\": 15,\n",
    "      \"narration\": \"This case explains whether Party A can continue living in a property claimed as a shared household.\",\n",
    "      \"visual\": \"White text on black background: Scene 1 overview\"\n",
    "    },\n",
    "    {\n",
    "      \"scene_number\": 2,\n",
    "      \"duration_seconds\": 20,\n",
    "      \"narration\": \"Party A argued residence rights, while Party B relied on ownership documents and sought possession.\",\n",
    "      \"visual\": \"White text on black background: Scene 2 arguments\"\n",
    "    },\n",
    "    {\n",
    "      \"scene_number\": 3,\n",
    "      \"duration_seconds\": 25,\n",
    "      \"narration\": \"The court focused on ownership proof and whether a domestic relationship existed after divorce.\",\n",
    "      \"visual\": \"White text on black background: Scene 3 reasoning\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "print(json.dumps(sample_gemini_output, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba3f6d",
   "metadata": {},
   "source": [
    "## 5) Evaluation & Analysis\n",
    "\n",
    "### a. Metrics used (quantitative or qualitative)\n",
    "\n",
    "This system was evaluated using functional and qualitative metrics that directly reflect whether the automation achieves its goal:\n",
    "\n",
    "Functional success metrics:\n",
    "- Shotstack returns status done\n",
    "- a final output video URL is generated successfully\n",
    "\n",
    "Scene correctness metrics:\n",
    "- the final video contains multiple scenes\n",
    "- scenes appear in correct order\n",
    "- the output is one combined video link rather than multiple fragmented links\n",
    "\n",
    "Timing correctness metrics:\n",
    "- each scene duration matches duration_seconds\n",
    "- start times accumulate properly (0, 5, 15, 27...) and scenes do not overlap incorrectly\n",
    "\n",
    "Content quality metrics:\n",
    "- narration remains neutral and educational\n",
    "- narration covers facts, issues, arguments, and reasoning\n",
    "- language is simplified for accessibility\n",
    "\n",
    "### b. Sample outputs / predictions\n",
    "\n",
    "The pipeline produces:\n",
    "- one Shotstack render ID\n",
    "- one final MP4 URL as output\n",
    "- a multi-scene text-based explainer video where each scene appears sequentially\n",
    "- consistent visual format (white text on black background)\n",
    "\n",
    "### c. Performance analysis and limitations\n",
    "\n",
    "Strengths:\n",
    "- fully automated end-to-end pipeline\n",
    "- multi-scene output works correctly\n",
    "- one final video link per judgment\n",
    "- scalable workflow for repeated use\n",
    "\n",
    "Limitations in the submitted version:\n",
    "- audio is not added yet\n",
    "- visuals are minimal and text-based\n",
    "- output quality depends on Gemini summarization consistency\n",
    "- complex judgments may require additional prompt refinement for best scene structuring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd3810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Shotstack response (demo only)\n",
    "sample_shotstack_response = {\n",
    "  \"render_id\": \"render_1234567890\",\n",
    "  \"status\": \"done\",\n",
    "  \"final_video_url\": \"https://example.com/final_video.mp4\"\n",
    "}\n",
    "\n",
    "import json\n",
    "print(json.dumps(sample_shotstack_response, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7dfa8",
   "metadata": {},
   "source": [
    "## 6) Ethical Considerations & Responsible AI\n",
    "\n",
    "### a. Bias and fairness considerations\n",
    "\n",
    "Legal summarization can unintentionally introduce bias if the summary:\n",
    "- emphasizes one party’s narrative more than the other\n",
    "- omits key reasoning that affects interpretation\n",
    "- oversimplifies disputes in a misleading way\n",
    "\n",
    "This risk is reduced by:\n",
    "- keeping narration neutral and factual\n",
    "- using balanced explanations of both sides\n",
    "- avoiding emotional or accusatory language\n",
    "- using neutral labels like Party A and Party B instead of assuming relationships\n",
    "\n",
    "### b. Dataset limitations\n",
    "\n",
    "This project uses live judgment PDFs rather than a fixed dataset. Therefore, input quality varies widely:\n",
    "- scanned PDFs may produce weaker extraction quality\n",
    "- formatting differences across courts reduce consistency\n",
    "- some judgments may omit structured headings and require deeper inference\n",
    "\n",
    "### c. Responsible use of AI tools\n",
    "\n",
    "The system is designed for educational understanding and simplified explanation. It should not be treated as a replacement for reading the original judgment or for professional legal interpretation. The output should be used as a learning aid and a quick explainer rather than as legal advice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0bda0",
   "metadata": {},
   "source": [
    "## 7) Conclusion & Future Scope\n",
    "\n",
    "### a. Summary of results\n",
    "\n",
    "We successfully built a working Make.com automation pipeline that:\n",
    "- takes a judgment PDF as input\n",
    "- extracts text automatically\n",
    "- uses Gemini to generate a structured multi-scene script\n",
    "- iterates over the scenes and aggregates them into one Shotstack timeline\n",
    "- generates one final video URL containing multiple scenes in sequence\n",
    "\n",
    "The current submitted version works end-to-end without audio, and successfully produces a multi-scene explainer video output with a single final link.\n",
    "\n",
    "### b. Possible improvements and extensions\n",
    "\n",
    "Future improvements planned after this stage include:\n",
    "- adding voiceover using ElevenLabs (text-to-speech)\n",
    "- enabling multilingual audio generation for different languages\n",
    "- adding translation features so judgments in any language can be converted into explainer videos in the same language\n",
    "- adding background visuals or images per scene for a more animated feel\n",
    "- adding subtitles and improved typography\n",
    "- adding background music at low volume\n",
    "- implementing automatic polling for Shotstack status instead of manual reruns\n",
    "- adding a fallback JSON repair step if Gemini output fails formatting\n",
    "- storing the final video URL back into Bubble database for the LawStory AI interface\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
